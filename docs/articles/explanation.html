<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>A tidy explanation of the Naive Bayes classifier â€¢ tidynaivebayes</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">tidynaivebayes</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li>
  <a href="../articles/explanation.html">Explanation</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/datascienceworkshops/tidynaivebayes">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>A tidy explanation of the Naive Bayes classifier</h1>
                        <h4 class="author">Jeroen Janssens</h4>
            
            <h4 class="date">2017-08-01</h4>
          </div>

    
    
<div class="contents">
<!-- # https://en.wikipedia.org/wiki/Bayesian_inference -->
<!-- ```{r, results="hide", message=FALSE, warning=FALSE} -->
<!-- library(tidyverse) -->
<!-- ``` -->
<!-- ## Even tidier data -->
<!-- Tidy data is  -->
<!-- Each variable is a column -->
<!-- Each observation is a row -->
<!-- Each type of observational unit is a table -->
<!-- In that sense, iris is already tidy. -->
<!-- But we will go one step further, where each measurement because a row. -->
<!-- This may seem odd at first.  -->
<!-- Makes the use of the functions in dplyr and tidyr possible. -->
<!-- Just like a data frame of tidy text, one token per row: -->
<!-- ```{r} -->
<!-- library(tidytext) -->
<!-- library(janeaustenr) -->
<!-- words <- austen_books() %>% -->
<!--   unnest_tokens(word, text) %>% -->
<!--   group_by(book, word) %>% -->
<!--   summarize(n = n()) %>% -->
<!--   ungroup() %>% -->
<!--   bind_tf_idf(word, book, n) %>% -->
<!--   select(book, word, tf_idf) %>% -->
<!--   arrange(desc(tf_idf)) -->
<!-- ``` -->
<!-- ```{r, echo = FALSE} -->
<!-- knitr::kable(head(words, 10), caption = "First ten rows of data frame `flowers`.") -->
<!-- ``` -->
<!-- From observations to a data frame of measurements -->
<!-- ```{r} -->
<!-- data(iris) -->
<!-- flowers <- iris %>% -->
<!--   as_data_frame() %>% -->
<!--   janitor::clean_names() %>% -->
<!--   rename(class = species) -->
<!-- ``` -->
<!-- ```{r, echo = FALSE} -->
<!-- knitr::kable(head(iris, 5), caption = "First five rows of data frame `flowers`.") -->
<!-- ``` -->
<!-- ```{r, results="hide", message=FALSE, warning=FALSE} -->
<!-- # Create tidy data set -->
<!-- measurements <- iris %>% -->
<!--   mutate(id = row_number()) %>% -->
<!--   gather(key = feature, value = value, -id, -class) %>% -->
<!--   select(id, feature, value, class) %>% -->
<!--   arrange(id) -->
<!-- ``` -->
<!-- ```{r, echo = FALSE} -->
<!-- knitr::kable(head(flower_measurements, 8), caption = "First eight rows of data frame `measurements`.") -->
<!-- ``` -->
<!-- ## Bayes theorem -->
<!-- $$ p(\textrm{class} \mid \textrm{data}) = \frac{p(\textrm{data} \mid \textrm{class}) \times p(\textrm{class})}{p(\textrm{data})} $$ -->
<!-- * $\textrm{class}$ is a particular class (e.g. setosa) -->
<!-- * $\textrm{data}$ are the feature values of a data point  -->
<!-- * $p(\textrm{class} \mid \textrm{data})$ is called the posterior -->
<!-- * $p(\textrm{data}  \mid \textrm{class})$ is called the likelihood -->
<!-- * $p(\textrm{class})$ is called the prior -->
<!-- * $p(\textrm{data})$ is called the marginal probability -->
<!-- ## Naive Bayes classifier -->
<!-- $$ p(data \mid \textrm{setosa}) = p(\textrm{setosa}) \\ \times p(\textrm{sepal_length} \mid \textrm{setosa}) \times p(\textrm{sepal_width} \mid \textrm{setosa}) \\ \times p(\textrm{petal_length} \mid \textrm{setosa}) \times p(\textrm{petal_width} \mid \textrm{setosa}) $$  -->
<!-- * Features are independent -->
<!-- * Denominator is ignored -->
<!-- * One equation per class -->
<!-- * Maximum numerator postorior is important -->
<!-- ## Visualize features -->
<!-- ```{r, echo=FALSE} -->
<!-- ggplot(df, aes(value, fill = class)) + -->
<!--   geom_density(alpha = 0.6) + -->
<!--   facet_grid(feature ~ ., scales = "free_y") -->
<!-- ``` -->
<!-- ## Gaussian Naive Bayes -->
<!-- $$p(x=v \mid c)=\frac{1}{\sqrt{2\pi\sigma^2_c}}\,e^{ -\frac{(v-\mu_c)^2}{2\sigma^2_c} }$$ -->
<!-- ```{r} -->
<!-- values <- c(1, 3, 2, 3, 2, 3, 1, 4) -->
<!-- v <- 4.1 -->
<!-- 1 / (sqrt(2 * pi * var(values))) * exp(-(v - mean(values))^2 / (2 * var(values))) -->
<!-- v <- 2.5 -->
<!-- 1 / (sqrt(2 * pi * var(values))) * exp(-(v - mean(values))^2 / (2 * var(values))) -->
<!-- ``` -->
<!-- ## Compute $\mu$ and $\sigma$ from training data -->
<!-- ```{r} -->
<!-- # Split the data -->
<!-- train_ids <- data_frame(id = sample(max(df$id), max(df$id) * 0.8)) -->
<!-- head(train_ids, 3) -->
<!-- df_train <- semi_join(df, train_ids, by = "id") -->
<!-- df_test <- anti_join(df, train_ids, by = "id") -->
<!-- # Compute mean and variance for each species and each feature -->
<!-- stats <- df_train %>% -->
<!--   group_by(class, feature) %>% -->
<!--   summarize(average = mean(value), variance = var(value)) %>% -->
<!--   ungroup() -->
<!-- ``` -->
<!-- ## Compute $\mu$ and $\sigma$ from training data -->
<!-- ```{r, echo = FALSE} -->
<!-- knitr::kable(head(stats)) -->
<!-- ``` -->
<!-- ## Visualize transformed values with test point -->
<!-- ```{r, eval=FALSE} -->
<!-- crossing(feature = unique(df$feature), value = seq(0, 8, by = 0.01)) %>% -->
<!--   inner_join(stats, by = "feature") %>% -->
<!--   mutate(p = 1 / (sqrt(2 * pi * variance)) * exp((-(value - average)^2) / (2 * variance))) %>% -->
<!--   ggplot(aes(value, p, fill = class)) + geom_density(stat = "identity", alpha = 0.6) + -->
<!--   geom_point(data = test_point, aes(value), y = 0) + facet_grid(feature ~ .) -->
<!-- test_point <- filter(df, id == min(df_test$id)) -->
<!-- ``` -->
<!-- ```{r, echo=FALSE} -->
<!-- test_point <- filter(df, id == min(df_test$id)) -->
<!-- knitr::kable(test_point) -->
<!-- ``` -->
<!-- ## Visualize transformed values with test point -->
<!-- ```{r, echo=FALSE} -->
<!-- test_point <- filter(df, id == min(df_test$id)) -->
<!-- crossing(feature = unique(df$feature), -->
<!--          value = seq(0, 8, by = 0.01)) %>% -->
<!--   inner_join(stats, by = "feature") %>% -->
<!--   mutate(p = 1 / (sqrt(2 * pi * variance)) * exp((-(value - average)^2) / (2 * variance))) %>% -->
<!--   ggplot(aes(value, p, fill = class)) + -->
<!--   geom_density(stat = "identity", alpha = 0.6) + -->
<!--   geom_point(data = test_point, aes(value), y = 0) + -->
<!--   facet_grid(feature ~ .) -->
<!-- ``` -->
<!-- ## Compute likelihoods of test data -->
<!-- ```{r} -->
<!-- likelihoods <- df_test %>% select(-class) %>% -->
<!--   inner_join(stats, by = "feature") %>% -->
<!--   mutate(p = 1 / (sqrt(2 * pi * variance)) * exp((-(value - average)^2) / (2 * variance))) -->
<!-- filter(likelihoods, id == min(df_test$id)) -->
<!-- ``` -->
<!-- ## Compute total likelihood -->
<!-- ```{r} -->
<!-- likelihood <- -->
<!--   likelihoods %>% -->
<!--   group_by(id, class) %>% -->
<!--   summarise(likelihood = prod(p)) %>% -->
<!--   ungroup() -->
<!-- head(likelihood, n = 6) -->
<!-- ``` -->
<!-- ## Compute priors $p(\textrm{class})$ -->
<!-- ```{r} -->
<!-- priors <- df_train %>% -->
<!--   group_by(class) %>% -->
<!--   summarise(prior = n() / nrow(df_train)) -->
<!-- ``` -->
<!-- ```{r, echo=FALSE} -->
<!-- knitr::kable(priors) -->
<!-- ``` -->
<!-- ## Compute unnormalized posterior -->
<!-- ```{r} -->
<!-- posteriors <- -->
<!--   likelihood %>% -->
<!--   inner_join(priors, by = "class") %>% -->
<!--   mutate(posterior = prior * likelihood) -->
<!-- head(posteriors, n = 6) -->
<!-- ``` -->
<!-- ## Select for each id the class with highest posterior -->
<!-- ```{r} -->
<!-- predictions <- -->
<!--   posteriors %>% -->
<!--   arrange(desc(posterior)) %>% -->
<!--   group_by(id) %>% -->
<!--   summarize(prediction = first(class), -->
<!--             posterior = first(posterior)) -->
<!-- sample_n(predictions, 6) -->
<!-- ``` -->
<!-- ## In summary, Naive Bayes -->
<!-- * Works with missing data -->
<!-- * Supports more than two classes -->
<!-- * Has no parameters -->
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by <a href="http://jeroenjanssens.com">Jeroen Janssens</a>, <a href="https://www.datascienceworkshops.com"><img src="https://datascienceworkshops.github.io/tidynaivebayes/data-science-workshops.svg" height="50px" style="vertical-align: baseline; margin-bottom: -7px"></a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
